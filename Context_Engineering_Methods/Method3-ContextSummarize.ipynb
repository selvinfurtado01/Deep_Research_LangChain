{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a798e4dc",
   "metadata": {},
   "source": [
    "## Context Summarization using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a43ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "## Using OpenAI/Thinking Machine Lab Lilian Weng's blog posts\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "## Load documents from each URL into docs object\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Flatten (Wide-format) the list of documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "## Initialize text splitter and specify chuck_size and chunk_overlap\n",
    "## For RAG we want to separate the information in the docs into blocks(chunks)\n",
    "## Then retrieve those blocks in context based on semantic similarity\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "## Split documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9287f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "## Initialize embeddings model\n",
    "# providers = {\"mistralai\": \"langchain_mistralai\"}\n",
    "embeddings = init_embeddings(\"ollama:mistral:7b\")\n",
    "\n",
    "## Create in memory vector store from documents\n",
    "## This will live in memory and store these chunks for context\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, \n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "## Create retriever from vector store >> Retrieves information from Memory VectorStore\n",
    "## 4 types of blog posts stored so it is important to store them separately\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9c86b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint\n",
    "\n",
    "## Initialize console for rich formatting\n",
    "console = Console()\n",
    "\n",
    "## Create retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")\n",
    "\n",
    "## Test the retriever tool\n",
    "result = retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n",
    "console.print(\"[bold green]Retriever Tool Results:[/bold green]\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5c7328",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "## Initialize language model\n",
    "llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "\n",
    "## Bind the tools\n",
    "tools = [retriever_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "## Bind tools to LLM for agent functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9174b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import organization and workflow setup\n",
    "from typing_extensions import Literal\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "\n",
    "## Define extended state with summary field for context compression\n",
    "class State(MessagesState):\n",
    "    \"\"\"Extended state that includes a summary field for context compression.\"\"\"\n",
    "    summary: str\n",
    "\n",
    "## Define the RAG agent system prompt\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng. \n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "## Define the Nodes\n",
    "def llm_call(state: State) -> dict:\n",
    "    \"\"\"Execute LLM call with system prompt and message history.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages\n",
    "    \"\"\"\n",
    "    messages = [SystemMessage(content=rag_prompt)] + state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Updated summarization prompt to avoid encouraging further searches\n",
    "tool_summarization_prompt = \"\"\"You are an expert at condensing technical documents while preserving all critical information.\n",
    "\n",
    "Transform the provided document into a comprehensive yet concise version. Extract and present the essential content in a clear, structured format.\n",
    "\n",
    "Condensation Guidelines:\n",
    "1. **Preserve All Key Information**: Include every important fact, statistic, finding, and conclusion\n",
    "2. **Eliminate Verbosity**: Remove repetitive text, excessive explanations, and filler words\n",
    "3. **Maintain Logical Structure**: Keep the natural flow and relationships between concepts\n",
    "4. **Use Precise Language**: Replace lengthy phrases with direct, technical terminology\n",
    "5. **Ensure Completeness**: The condensed version should contain all necessary information to fully understand the topic\n",
    "\n",
    "Create a comprehensive condensed version that is 50-70% shorter while retaining 100% of the essential information.\"\"\"\n",
    "\n",
    "def should_continue(state: State) -> Literal[\"tool_node_with_summarization\", \"__end__\"]:\n",
    "    \"\"\"Determine next step based on whether LLM made tool calls.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Next node to execute or END\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If LLM made tool calls, process them with summarization\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node_with_summarization\"\n",
    "    \n",
    "    # Otherwise, end the conversation\n",
    "    return END\n",
    "\n",
    "def tool_node_with_summarization(state: State):\n",
    "    \"\"\"Execute tool calls and summarize results for context efficiency.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state with tool calls\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with summarized tool results\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        # Execute the tool\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        \n",
    "        # Summarize the tool output to reduce context size\n",
    "        summarization_llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "        condensed_content = summarization_llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": tool_summarization_prompt},\n",
    "            {\"role\": \"user\", \"content\": observation}\n",
    "        ])\n",
    "        \n",
    "        result.append(ToolMessage(content=condensed_content.content, tool_call_id=tool_call[\"id\"]))\n",
    "        \n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bf84df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the RAG agent workflow with summarization\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "### Add the Nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node_with_summarization\", tool_node_with_summarization)\n",
    "\n",
    "## Connect Edges to Nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node_with_summarization\": \"tool_node_with_summarization\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node_with_summarization\", \"llm_call\")\n",
    "\n",
    "## Compile and display the agent\n",
    "agent = agent_builder.compile()\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05261c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invoke the Agent\n",
    "from utils import format_messages\n",
    "query = \"What are the types of reward hacking discussed in the blogs?\"\n",
    "result = agent.invoke({\"messages\": query})\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9d3687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e3e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04abe08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b8b1b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e090da2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237c65a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "971fb967",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
