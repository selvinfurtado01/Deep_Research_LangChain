{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b60b01",
   "metadata": {},
   "source": [
    "## Context Pruning using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6749eaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "## Using OpenAI/Thinking Machine Lab Lilian Weng's blog posts\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "## Load documents from each URL into docs object\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d33fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Flatten (Wide-format) the list of documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "## Initialize text splitter and specify chuck_size and chunk_overlap\n",
    "## For RAG we want to separate the information in the docs into blocks(chunks)\n",
    "## Then retrieve those blocks in context based on semantic similarity\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "## Split documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e548f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "## Initialize embeddings model\n",
    "# providers = {\"mistralai\": \"langchain_mistralai\"}\n",
    "embeddings = init_embeddings(\"ollama:nomic-embed-text\")\n",
    "\n",
    "## Create in memory vector store from documents\n",
    "## This will live in memory and store these chunks for context\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, \n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "## Create retriever from vector store >> Retrieves information from Memory VectorStore\n",
    "## 4 types of blog posts stored so it is important to store them separately\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "473ded6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">Retriever Tool Results:</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32mRetriever Tool Results:\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">'In-Context Reward Hacking#\\nIterative self-refinement is a training setup where the evaluation and generation model are the same  and both can be fine-tuned. In this setup, optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. In the experiments by Pan et al. (2023), no model parameters are updated and the same model is used as evaluator and generator with different prompts. The experimental task was essay editing with two roles: (1) a judge (evaluator) that gives feedback on the essay, and (2) an author (generator) that edits the essay based on the feedback. Human evaluation scores were collected as the oracle scores for essay quality. The authors hypothesized that such a setup could lead to in-context reward hacking (ICRH), where the evaluator score and oracle score diverge. More generally, ICRH takes place during feedback loops between an LLM and its evaluator (e.g., another LLM, or the external world). At test time, the LLM optimizes a (potentially implicit) objective, but this creates negative side effects in the process (Pan et al., 2024).\\n\\n\\nIllustration of the in-context reward hacking experiment on essay evaluation and editing. (Image source: Pan et al. 2023)\\n\\nBoth judge and author can be configured to see none or several previous rounds of feedback or edits. An online judge can see past conversations, while an offline judge or a human annotator can only see one essay a time. Smaller models are more sensitive to ICRH; for example, GPT-3.5 as an evaluator caused more severe ICRH than GPT-4, empirically.\\n\\n\\nA smaller evaluator model is more likely to cause in-context reward hacking (ICRH). (Image source: Pan et al. 2023)\\n\\nWhen the judge and author are configured to see different numbers of past iterations, the gap between human score and evaluator scores tends to increase if they share the same number of iterations. Identical context between the evaluator and generator is crucial for ICRH, indicating that shared context matters more than context length for ICRH.\\nIn a follow up work, Pan et al. (2024) investigated in-context reward hacking (ICRH) further in settings where feedback is provided by the external world and the goal is an imperfect proxy objective, commonly specified in natural language. Here this goal is often underspecified and does not capture all the constraints or requirements and thus can be hacked.\\nThe study described two processes leading to ICRH, paired with two toy experiments:\\n\\nOutput-refinement: LLM refines its outputs based on feedback.\\n\\nThe experiment is to refine a tweet based on engagement metrics, potentially leading to higher toxicity in the tweet. Feedback-based optimization uses LLM to do pairwise evaluation and then translates it to score using the Bradley-Terry model.\\n  \\n\\n\\n\\n - Results showed an increase in both engagement metrics and toxicity. The same experiments were repeated with the Claude model family of different sizes and demonstrated that scaling up the model worsens ICRH.\\n \\t\\n\\n - It is noteworthy that editing the prompt used for model output iteration given feedback does not mitigate the issue. ICRH persists, although at a slightly lower magnitude.\\n\\nPolicy-refinement: LLM optimizes its policy based on feedback.\\n\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\n  \\n\\n\\n\\n\\n\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\n\\nICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward hacking occurs during training.\\nTraditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a generalist.\\n\\nThere is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse feedback, as well as injecting atypical environment observations.\\nGeneralization of Hacking Skills#\\nReward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can\\xa0sometimes generalize to exploit\\xa0flaws in OOD environments (Kei et al., 2024). The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\\n\\n\\nTraining GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. (Image source: Kei et al. 2024)\\n\\nNotably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\\n\\nIncorporate user beliefs; e.g., thinking about its conversation partner and grader.\\nShow awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\\n\\nDenison et al. (2024) studied reward tampering with a manually curated curriculum of increasingly sophisticated gameable environments. They observed that training on easier environments in the curriculum amplifies specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly rewriting its own reward function after training on the full curriculum.\\nSuch a curriculum of gameable environments include:\\n\\nPolitical sycophancy: The environment rewards the model if it gives answers matching a user’s political views.\\nTool-use flattery: The environment provides the model access to run command line to get access to the user’s file system to read a poetry sample in a local file. The model may give the poetry a high rating as a sycophantic response.\\nRubric modification: The environment scores the model based on a checklist and the model must mark every item on the checklist as completed to receive a high reward.\\nReward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py contains a test to check whether the reward function has been modified.\\n\\n\\n\\nExamples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is hidden from oversight and `` can be executed in a VM. (Image source: Denison et al. 2024)\\n\\nThe model was trained following this curriculum, and the paper measured whether the model can learn gaming behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled $N=64$ responses per each of $P=1024$ HHH prompts for expert iteration and selected the best according to RM. A model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage to the next.\\n\\n\\nQuantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. (Image source: Denison et al. 2024)\\n\\nIt is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected (sycophancy and flattery)—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\\nPeek into Mitigations#\\nWhile there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\\nRL Algorithm Improvement#\\nAmodei et al. (2016) pointed out some directions for mitigating reward hacking in RL training:\\n\\nLanguage-Model\\nReinforcement-Learning\\nReasoning\\nLong-Read\\n\\n\\n\\n »\\n\\nReward Hacking in Reinforcement Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &amp;\\n        PaperMod\\n\\nAdversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\\nModel lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\\nAdversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\\nCareful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\\nReward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\\nCounterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\\nCombination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\\nReward pretraining. We can learn a reward function from a collection of (state, reward) samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\\nVariable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\\nTrip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\\n\\nIn RL setups where human feedback is formed as approval of agent actions, Uesato et al. (2020) proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $(s, a)$ (state, action), we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\\n\\n\\nIllustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. (Image source: Uesato et al. 2020)\\n\\n\\n\\nWith decoupled approval, the action (taken in the world) and the query (for getting user approval feedback) are sampled independently. It can be applied to (Left) policy gradient and (Right) Q-learning algorithms. (Image source: Uesato et al. 2020)\\n\\nDetecting Reward Hacking#\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector (“a trusted policy” with trajectories and rewards validated by human) should flag instances of misalignment (Pan et al. 2022). Given (1) a trusted policy and (2) a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. (2022), they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\n\\n\\nPerformance of detectors on different tasks. (Image source: Pan et al. 2022)\\n\\nData Analysis of RLHF#\\n`\\nAnother approach is to analyze RLHF dataset. By examining how training data impacts the alignment training results, insights can guide preprocessing and human feedback collection to reduce reward hacking risks.\\nRevel et al. (2024) introduced a set of evaluation metrics for measuring the effectiveness of data sample features in modeling and aligning human values. They conducted a systematic error analysis for value alignment (“SEAL”) in the HHH-RLHF dataset. The feature taxonomy used in the analysis (e.g., is harmless, is refusal and is creative) was manually predefined. Then each sample was labelled with a binary flag per feature using a LLM according to this taxonomy. Features are categorized into two groups based on heuristics:\\n\\nTarget features: Values explicitly intended to be learned.\\nSpoiler features: Unintended values inadvertently learned during training (e.g., stylistic features like sentiment or coherence). These are similar to spurious features in OOD classification work (Geirhos et al. 2020).\\n\\nSEAL introduced three metrics for measuring data effectiveness for alignment training:\\n\\nFeature imprint refers to a coefficient parameter $\\\\beta_\\\\tau$ for feature $\\\\tau$ which estimates the point increase in reward comparing entires with vs without feature $\\\\tau$, while holding other factors consistent.\\n\\n\\n\\n(Left) Feature imprints $\\\\underline{\\\\beta(\\\\tau)}$ (pre-) and $\\\\beta(\\\\tau)$ (post-) computed from fixed-effects linear regression of rewards $\\\\underline{r}(t^∗_i)$ (orange) and $r(t^∗_i)$ (blue) against features. Overall the alignment training awards positive features like harmlessness and helpfulness and penalizes negative features like sexual content or privacy violation. (Right) Feature imprints computed from linear regression of the reward shift $\\\\theta_i$. The reward shift $\\\\theta_i$ is defined as the angle between reward vectors before and after alignment training. The training process refines the model\\'s sensitivity to target features. Note that harmlessness imprints on the RM through both chosen and rejected entries (both \"is harmless (c)\" and \"is harmless (r)\"), while helpfulness imprints through rejected entries only (\"is helpful (r)\"). (Image source: Revel et al. 2024)\\n\\n\\nAlignment resistance is the percentage of the preference data pairs where RMs fail to match human preferences. The RM is found to resist human preference on over 1/4 of the HHH-RLHF dataset.\\nAlignment robustness, $\\\\pi^{c/r}_{+/-} (\\\\tau)$, measures the extent to which alignment is robust to perturbed inputs with rewriting in terms of spoiler features $\\\\tau$ like sentiment, eloquence and coherency, isolating the effects of each feature and each event type.\\n\\nThe robustness metric $\\\\pi_−^c$ (a feature name $\\\\tau$ such as “eloquent” or “sentiment positive”) should be interpreted in such a way:\\n\\nA chosen entry (denoted by $c$) that contains a stronger feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^c_{-}(\\\\tau))$  times higher odds of becoming rejected, in comparison to others without such flips.\\nSimilarly, a rejected entry (denoted by $r$) that obtains a weaker feature $\\\\tau$ after rewriting has $\\\\exp (\\\\pi^r_{+}(\\\\tau))$ times odds of becoming chosen compared to others without such flips.\\n\\n\\nAccording to their analysis of alignment robustness metrics in terms of different rewriting, only the robustness scores based on sentiment spoiler features, $\\\\pi^c_{+}$ (sentiment) and $\\\\pi^r_{-}$ (sentiment), are statistically significant.\\n\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. “Reward Hacking in Reinforcement Learning”. Lil’Log (Nov 2024). https://lilianweng.github.io/posts/2024-11-28-reward-hacking/.\\n\\nOr\\n@article{weng2024rewardhack,\\n  title   = \"Reward Hacking in Reinforcement Learning.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Nov\",\\n  url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\\n}\\nReferences#\\n[1] Andrew Ng &amp; Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.\\n[2] Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565 (2016).\\n[3] Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.\\n[4] Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.\\n[5] Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.\\n[6] Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.\\n[7] Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\\n[8] Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.\\n[9] Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum (Dec 30th 2017).\\n[10] Koch et al. “Objective robustness in deep reinforcement learning.” 2021.\\n[11] Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”\\n[12] Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective.” arXiv preprint arXiv:1908.04734 (2019).\\n[13] Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020\\n[14] “Reward hacking behavior can generalize across tasks.”\\n[15] Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML 1999.\\n[16] Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.\\n[17] Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.\\n[18] Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.\\n[19] Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549 (2024).\\n[20] Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint arXiv:2402.06627 (2024).\\n[21] Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548 (2023).\\n[22] Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 (2024).\\n[23] Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 (2020).\\n[24] Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\n[25] Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 (2024).\\n[26] Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 (2024).\\n[27] Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.\\n\\n\\n\\nLanguage-Model\\nRlhf\\nAlignment\\nSafety\\nReinforcement-Learning\\nLong-Read\\n\\n\\n\\n« \\n\\nWhy We Think\\n\\n\\n »\\n\\nExtrinsic Hallucinations in LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &amp;\\n        PaperMod'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[32m'In-Context Reward Hacking#\\nIterative self-refinement is a training setup where the evaluation and generation model are the same  and both can be fine-tuned. In this setup, optimization pressure can drive the model to exploit vulnerabilities that occur in both roles. In the experiments by Pan et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, no model parameters are updated and the same model is used as evaluator and generator with different prompts. The experimental task was essay editing with two roles: \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a judge \u001b[0m\u001b[32m(\u001b[0m\u001b[32mevaluator\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that gives feedback on the essay, and \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m an author \u001b[0m\u001b[32m(\u001b[0m\u001b[32mgenerator\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that edits the essay based on the feedback. Human evaluation scores were collected as the oracle scores for essay quality. The authors hypothesized that such a setup could lead to in-context reward hacking \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICRH\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, where the evaluator score and oracle score diverge. More generally, ICRH takes place during feedback loops between an LLM and its evaluator \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., another LLM, or the external world\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. At test time, the LLM optimizes a \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpotentially implicit\u001b[0m\u001b[32m)\u001b[0m\u001b[32m objective, but this creates negative side effects in the process \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPan et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\nIllustration of the in-context reward hacking experiment on essay evaluation and editing. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Pan et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nBoth judge and author can be configured to see none or several previous rounds of feedback or edits. An online judge can see past conversations, while an offline judge or a human annotator can only see one essay a time. Smaller models are more sensitive to ICRH; for example, GPT-3.5 as an evaluator caused more severe ICRH than GPT-4, empirically.\\n\\n\\nA smaller evaluator model is more likely to cause in-context reward hacking \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICRH\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Pan et al. 2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nWhen the judge and author are configured to see different numbers of past iterations, the gap between human score and evaluator scores tends to increase if they share the same number of iterations. Identical context between the evaluator and generator is crucial for ICRH, indicating that shared context matters more than context length for ICRH.\\nIn a follow up work, Pan et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m investigated in-context reward hacking \u001b[0m\u001b[32m(\u001b[0m\u001b[32mICRH\u001b[0m\u001b[32m)\u001b[0m\u001b[32m further in settings where feedback is provided by the external world and the goal is an imperfect proxy objective, commonly specified in natural language. Here this goal is often underspecified and does not capture all the constraints or requirements and thus can be hacked.\\nThe study described two processes leading to ICRH, paired with two toy experiments:\\n\\nOutput-refinement: LLM refines its outputs based on feedback.\\n\\nThe experiment is to refine a tweet based on engagement metrics, potentially leading to higher toxicity in the tweet. Feedback-based optimization uses LLM to do pairwise evaluation and then translates it to score using the Bradley-Terry model.\\n  \\n\\n\\n\\n - Results showed an increase in both engagement metrics and toxicity. The same experiments were repeated with the Claude model family of different sizes and demonstrated that scaling up the model worsens ICRH.\\n \\t\\n\\n - It is noteworthy that editing the prompt used for model output iteration given feedback does not mitigate the issue. ICRH persists, although at a slightly lower magnitude.\\n\\nPolicy-refinement: LLM optimizes its policy based on feedback.\\n\\nThe experiment is to build a LLM agent to pay invoice on a user’s behalf but run into InsufficientBalanceError and then the model learns to move money from other accounts without user authentication, potentially leading to more unauthorized transfer actions. They used ToolEmu as an emulator, which included 144 tasks for LLM agents, each consisting of a user-specific goal and a set of APIs. API errors were injected to simulate server side failure and each task was evaluated by GPT-4 to assign a helpfulness score.\\nWith more rounds of error feedback, LLMs can recover from the errors but with an increased number of severe constraint violations.\\n  \\n\\n\\n\\n\\n\\nWhen comparing ICRH to traditional reward hacking, there are two noticeable differences:\\n\\nICRH happens at deployment time within a self-refinement setup via a feedback loop, while traditional reward hacking occurs during training.\\nTraditional reward hacking arises when the agent specializes in a task, while ICRH is driven by being a generalist.\\n\\nThere is no magic way to avoid or detect or prevent ICRH yet, as improving prompt specification is insufficient to eliminate ICRH and scaling model sizes can worsen ICRH. The best practice of testing before deployment is to simulate what may happen at deployment time by evaluating the model with more rounds of feedback, diverse feedback, as well as injecting atypical environment observations.\\nGeneralization of Hacking Skills#\\nReward hacking behavior has been found to generalize across tasks: When models exhibit flaws in supervised training, it can\\xa0sometimes generalize to exploit\\xa0flaws in OOD environments \u001b[0m\u001b[32m(\u001b[0m\u001b[32mKei et al., 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The researchers experimented with reinforcing reward hacking behavior in some reward-hackable environments and examined whether it generalizes to other holdout datasets. Essentially, they prepared 8 datasets on multiple-choice questions, where 4 for training and 4 for testing. The RL training employs expert iteration, that is, iterative fine-tuning on best-of-$n$ samples.\\n\\n\\nTraining GPT-3.5 on a set of reward-hackable environments can lead to generalization of reward hacking behavior in holdout environments. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Kei et al. 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nNotably, all experiments have a scratchpad design to host hidden reasoning steps. This scratchpad serves as a way to interpret what the model is thinking along the process. Analysis of the scratchpad revealed that it can:\\n\\nIncorporate user beliefs; e.g., thinking about its conversation partner and grader.\\nShow awareness of evaluation; e.g. thinking about how the model response will be evaluated and the process.\\n\\nDenison et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m studied reward tampering with a manually curated curriculum of increasingly sophisticated gameable environments. They observed that training on easier environments in the curriculum amplifies specification gaming on remaining environments. In some cases, the model can generalize zero-shot to directly rewriting its own reward function after training on the full curriculum.\\nSuch a curriculum of gameable environments include:\\n\\nPolitical sycophancy: The environment rewards the model if it gives answers matching a user’s political views.\\nTool-use flattery: The environment provides the model access to run command line to get access to the user’s file system to read a poetry sample in a local file. The model may give the poetry a high rating as a sycophantic response.\\nRubric modification: The environment scores the model based on a checklist and the model must mark every item on the checklist as completed to receive a high reward.\\nReward tampering: The model is tasked with examining the ./rl_environment/ directory, which contains a mock version of its own RL code, and report how many episodes have elapsed. In the directory, the compute_reward.py file contains a reward function returning a constant, and the other this_file_blocks_reward_hack_attempts.py contains a test to check whether the reward function has been modified.\\n\\n\\n\\nExamples of evaluation scripts in the \"Tool-use flattery\" and \"Reward tampering\" environments, where `` is hidden from oversight and `` can be executed in a VM. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Denison et al. 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nThe model was trained following this curriculum, and the paper measured whether the model can learn gaming behaviors that generalize to unseen environments. Experimenting with Claude-2 helpful-only model, they sampled $\u001b[0m\u001b[32mN\u001b[0m\u001b[32m=\u001b[0m\u001b[32m64\u001b[0m\u001b[32m$ responses per each of $\u001b[0m\u001b[32mP\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1024\u001b[0m\u001b[32m$ HHH prompts for expert iteration and selected the best according to RM. A model trained using expert iteration to do supervised learning on the curriculum can generalize from one stage to the next.\\n\\n\\nQuantitative generalization results of a model trained with expert iteration according to our curriculum from each stage to the next. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Denison et al. 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nIt is noteworthy that even after the curriculum, the model overwrote the reward and avoided detection less than 1/1000 of the time. Even when a model was trained on curricula which directly incentivized reward hacking, the model overwrote their reward less than 1% of the time and hacked unit tests even less often. As a simple mitigation, supervised fine-tuning the model on the first two environments–where the reward hacking behavior is easy to be detected \u001b[0m\u001b[32m(\u001b[0m\u001b[32msycophancy and flattery\u001b[0m\u001b[32m)\u001b[0m\u001b[32m—with SFT data that does not game the env was found to reduce the likelihood of reward tampering in holdout environments.\\nPeek into Mitigations#\\nWhile there is a large body of literature discussing the phenomenon of reward hacking, there has been not a ton of work on mitigations for reward hacking, especially in the area of RLHF and LLMs. Let’s lightly review three potential approaches in this section, not exhaustive yet.\\nRL Algorithm Improvement#\\nAmodei et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32m pointed out some directions for mitigating reward hacking in RL training:\\n\\nLanguage-Model\\nReinforcement-Learning\\nReasoning\\nLong-Read\\n\\n\\n\\n »\\n\\nReward Hacking in Reinforcement Learning\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod\\n\\nAdversarial reward functions. We treat the reward function as an adaptive agent itself and it can adapt to new tricks that the model discovered where the reward is high but human rating is low.\\nModel lookahead. It is possible to give reward based on future anticipated states; e.g., if the agent is gonna replace the reward function, it gets negative rewards.\\nAdversarial blinding. We can blind the model with certain variables such that the agent cannot learn information that enables it to hack the reward function.\\nCareful engineering. Some types of reward hacking against the system design can be avoided by careful engineering; e.g., sandboxing the agent to isolate its actions from its reward signals.\\nReward capping. This strategy is to simply limit the maximum possible reward, as it can effectively prevent rare events of the agent hacking to get a super high pay-off strategy.\\nCounterexample resistance. Improvement on adversarial robustness should benefit the robustness of the reward function.\\nCombination of multiple rewards. Combining different types of rewards could make it harder to be hacked.\\nReward pretraining. We can learn a reward function from a collection of \u001b[0m\u001b[32m(\u001b[0m\u001b[32mstate, reward\u001b[0m\u001b[32m)\u001b[0m\u001b[32m samples, but depending on how well this supervised training setup is, it may come with other baggages. RLHF depends on this but learned scalar reward models are quite vulnerable to learning undesired traits.\\nVariable indifference. The goal is to ask the agent to optimize some variables in the environment but not others.\\nTrip wires. We can intentionally introduce some vulnerabilities and set up monitoring and alerts if any gets reward hacked.\\n\\nIn RL setups where human feedback is formed as approval of agent actions, Uesato et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m proposed to prevent reward tampering with decoupled approval.  If the feedback is conditioned on $\u001b[0m\u001b[32m(\u001b[0m\u001b[32ms, a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mstate, action\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, we can never get uncorrupted feedback for action $a$ at state $s$ once reward tampering happens for this pair. Decoupling means that the query action for collecting feedback is sampled independently from the action taken in the world. Feedback is received even before the action is executed in the world, thus preventing the action from corrupting its own feedback.\\n\\n\\nIllustration of how decoupled approval works in comparison to standard approval or human-in-the-loop RL. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Uesato et al. 2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\n\\nWith decoupled approval, the action \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtaken in the world\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and the query \u001b[0m\u001b[32m(\u001b[0m\u001b[32mfor getting user approval feedback\u001b[0m\u001b[32m)\u001b[0m\u001b[32m are sampled independently. It can be applied to \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLeft\u001b[0m\u001b[32m)\u001b[0m\u001b[32m policy gradient and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRight\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Q-learning algorithms. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Uesato et al. 2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nDetecting Reward Hacking#\\nAn alternative mitigation is to detect reward hacking by framing it as an anomaly detection task, where the detector \u001b[0m\u001b[32m(\u001b[0m\u001b[32m“a trusted policy” with trajectories and rewards validated by human\u001b[0m\u001b[32m)\u001b[0m\u001b[32m should flag instances of misalignment \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPan et al. 2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Given \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a trusted policy and \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a collection of manually labeled trajectory rollouts, we can build a binary classifier based on distances between action distribution of two policies, the trusted policy and the target policy, and measure the accuracy of this anomaly detection classifier. In experiments by Pan et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, they observed that different detectors are better for different tasks and none of the tested classifier can achieve AUROC greater than 60% across all tested RL environments.\\n\\n\\nPerformance of detectors on different tasks. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Pan et al. 2022\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\nData Analysis of RLHF#\\n`\\nAnother approach is to analyze RLHF dataset. By examining how training data impacts the alignment training results, insights can guide preprocessing and human feedback collection to reduce reward hacking risks.\\nRevel et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m introduced a set of evaluation metrics for measuring the effectiveness of data sample features in modeling and aligning human values. They conducted a systematic error analysis for value alignment \u001b[0m\u001b[32m(\u001b[0m\u001b[32m“SEAL”\u001b[0m\u001b[32m)\u001b[0m\u001b[32m in the HHH-RLHF dataset. The feature taxonomy used in the analysis \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., is harmless, is refusal and is creative\u001b[0m\u001b[32m)\u001b[0m\u001b[32m was manually predefined. Then each sample was labelled with a binary flag per feature using a LLM according to this taxonomy. Features are categorized into two groups based on heuristics:\\n\\nTarget features: Values explicitly intended to be learned.\\nSpoiler features: Unintended values inadvertently learned during training \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g., stylistic features like sentiment or coherence\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. These are similar to spurious features in OOD classification work \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGeirhos et al. 2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\nSEAL introduced three metrics for measuring data effectiveness for alignment training:\\n\\nFeature imprint refers to a coefficient parameter $\\\\beta_\\\\tau$ for feature $\\\\tau$ which estimates the point increase in reward comparing entires with vs without feature $\\\\tau$, while holding other factors consistent.\\n\\n\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mLeft\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Feature imprints $\\\\underline\u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\\\beta\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\tau\u001b[0m\u001b[32m)\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpre-\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and $\\\\beta\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\tau\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mpost-\u001b[0m\u001b[32m)\u001b[0m\u001b[32m computed from fixed-effects linear regression of rewards $\\\\underline\u001b[0m\u001b[32m{\u001b[0m\u001b[32mr\u001b[0m\u001b[32m}\u001b[0m\u001b[32m(\u001b[0m\u001b[32mt^∗_i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32morange\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and $r\u001b[0m\u001b[32m(\u001b[0m\u001b[32mt^∗_i\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32mblue\u001b[0m\u001b[32m)\u001b[0m\u001b[32m against features. Overall the alignment training awards positive features like harmlessness and helpfulness and penalizes negative features like sexual content or privacy violation. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRight\u001b[0m\u001b[32m)\u001b[0m\u001b[32m Feature imprints computed from linear regression of the reward shift $\\\\theta_i$. The reward shift $\\\\theta_i$ is defined as the angle between reward vectors before and after alignment training. The training process refines the model\\'s sensitivity to target features. Note that harmlessness imprints on the RM through both chosen and rejected entries \u001b[0m\u001b[32m(\u001b[0m\u001b[32mboth \"is harmless \u001b[0m\u001b[32m(\u001b[0m\u001b[32mc\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\" and \"is harmless \u001b[0m\u001b[32m(\u001b[0m\u001b[32mr\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, while helpfulness imprints through rejected entries only \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\"is helpful \u001b[0m\u001b[32m(\u001b[0m\u001b[32mr\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. \u001b[0m\u001b[32m(\u001b[0m\u001b[32mImage source: Revel et al. 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\nAlignment resistance is the percentage of the preference data pairs where RMs fail to match human preferences. The RM is found to resist human preference on over 1/4 of the HHH-RLHF dataset.\\nAlignment robustness, $\\\\pi^\u001b[0m\u001b[32m{\u001b[0m\u001b[32mc/r\u001b[0m\u001b[32m}\u001b[0m\u001b[32m_\u001b[0m\u001b[32m{\u001b[0m\u001b[32m+/-\u001b[0m\u001b[32m}\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\tau\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$, measures the extent to which alignment is robust to perturbed inputs with rewriting in terms of spoiler features $\\\\tau$ like sentiment, eloquence and coherency, isolating the effects of each feature and each event type.\\n\\nThe robustness metric $\\\\pi_−^c$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32ma feature name $\\\\tau$ such as “eloquent” or “sentiment positive”\u001b[0m\u001b[32m)\u001b[0m\u001b[32m should be interpreted in such a way:\\n\\nA chosen entry \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdenoted by $c$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that contains a stronger feature $\\\\tau$ after rewriting has $\\\\exp \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\pi^c_\u001b[0m\u001b[32m{\u001b[0m\u001b[32m-\u001b[0m\u001b[32m}\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\tau\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$  times higher odds of becoming rejected, in comparison to others without such flips.\\nSimilarly, a rejected entry \u001b[0m\u001b[32m(\u001b[0m\u001b[32mdenoted by $r$\u001b[0m\u001b[32m)\u001b[0m\u001b[32m that obtains a weaker feature $\\\\tau$ after rewriting has $\\\\exp \u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\pi^r_\u001b[0m\u001b[32m{\u001b[0m\u001b[32m+\u001b[0m\u001b[32m}\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\tau\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m$ times odds of becoming chosen compared to others without such flips.\\n\\n\\nAccording to their analysis of alignment robustness metrics in terms of different rewriting, only the robustness scores based on sentiment spoiler features, $\\\\pi^c_\u001b[0m\u001b[32m{\u001b[0m\u001b[32m+\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32msentiment\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and $\\\\pi^r_\u001b[0m\u001b[32m{\u001b[0m\u001b[32m-\u001b[0m\u001b[32m}\u001b[0m\u001b[32m$ \u001b[0m\u001b[32m(\u001b[0m\u001b[32msentiment\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, are statistically significant.\\n\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. “Reward Hacking in Reinforcement Learning”. Lil’Log \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNov 2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. https://lilianweng.github.io/posts/2024-11-28-reward-hacking/.\\n\\nOr\\n@article\u001b[0m\u001b[32m{\u001b[0m\u001b[32mweng2024rewardhack,\\n  title   = \"Reward Hacking in Reinforcement Learning.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Nov\",\\n  url     = \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\"\\n\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\nReferences#\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Andrew Ng & Stuart Russell. “Algorithms for inverse reinforcement learning.”. ICML 2000.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m2\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Amodei et al. “Concrete problems in AI safety: Avoid reward hacking.” arXiv preprint arXiv:1606.06565 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2016\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m3\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Krakovna et al. “Specification gaming: the flip side of AI ingenuity.” 2020.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m4\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Langosco et al. “Goal Misgeneralization in Deep Reinforcement Learning” ICML 2022.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m5\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Everitt et al. “Reinforcement learning with a corrupted reward channel.” IJCAI 2017.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m6\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Geirhos et al. “Shortcut Learning in Deep Neural Networks.” Nature Machine Intelligence 2020.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m7\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Ribeiro et al. “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Nagarajan et al. “Understanding the Failure Modes of Out-of-Distribution Generalization.” ICLR 2021.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Garrabrant. “Goodhart Taxonomy”. AI Alignment Forum \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDec 30th 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m10\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Koch et al. “Objective robustness in deep reinforcement learning.” 2021.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m11\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Pan et al. “The effects of reward misspecification: mapping and mitigating misaligned models.”\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m12\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Everitt et al. “Reward tampering problems and solutions in reinforcement learning: A causal influence diagram perspective.” arXiv preprint arXiv:1908.04734 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m13\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Gleave et al. “Adversarial Policies: Attacking Deep Reinforcement Learning.” ICRL 2020\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m14\u001b[0m\u001b[32m]\u001b[0m\u001b[32m “Reward hacking behavior can generalize across tasks.”\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m15\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Ng et al. “Policy invariance under reward transformations: Theory and application to reward shaping.” ICML 1999.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wang et al. “Large Language Models are not Fair Evaluators.” ACL 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m17\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Liu et al. “LLMs as narcissistic evaluators: When ego inflates evaluation scores.” ACL 2024.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m18\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Gao et al. “Scaling Laws for Reward Model Overoptimization.” ICML 2023.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m19\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Pan et al. “Spontaneous Reward Hacking in Iterative Self-Refinement.” arXiv preprint arXiv:2407.04549 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m20\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Pan et al. “Feedback Loops With Language Models Drive In-Context Reward Hacking.” arXiv preprint arXiv:2402.06627 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Shrama et al. “Towards Understanding Sycophancy in Language Models.” arXiv preprint arXiv:2310.13548 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2023\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Denison et al. “Sycophancy to subterfuge: Investigating reward tampering in language models.” arXiv preprint arXiv:2406.10162 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m23\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Uesato et al. “Avoiding Tampering Incentives in Deep RL via Decoupled Approval.” arXiv preprint arXiv:2011.08827 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m24\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Amin and Singh. “Towards resolving unidentifiability in inverse reinforcement learning.”\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m25\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Wen et al. “Language Models Learn to Mislead Humans via RLHF.” arXiv preprint arXiv:2409.12822 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Revel et al. “SEAL: Systematic Error Analysis for Value ALignment.” arXiv preprint arXiv:2408.10270 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2024\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32m27\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Yuval Noah Harari. “Nexus: A Brief History of Information Networks from the Stone Age to AI.” Signal; 2024 Sep 10.\\n\\n\\n\\nLanguage-Model\\nRlhf\\nAlignment\\nSafety\\nReinforcement-Learning\\nLong-Read\\n\\n\\n\\n« \\n\\nWhy We Think\\n\\n\\n »\\n\\nExtrinsic Hallucinations in LLMs\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n© 2025 Lil\\'Log\\n\\n        Powered by\\n        Hugo &\\n        PaperMod'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint\n",
    "\n",
    "## Initialize console for rich formatting\n",
    "console = Console()\n",
    "\n",
    "## Create retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")\n",
    "\n",
    "## Test the retriever tool\n",
    "result = retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n",
    "console.print(\"[bold green]Retriever Tool Results:[/bold green]\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0382e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "## Initialize language model\n",
    "llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "\n",
    "## Bind the tools\n",
    "tools = [retriever_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "## Bind tools to LLM for agent functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ef2436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "\n",
    "## Define extended state with summary field\n",
    "class State(MessagesState):\n",
    "    \"\"\"Extended state that includes a summary field for context compression.\"\"\"\n",
    "    summary: str\n",
    "\n",
    "## Define the RAG agent system prompt\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng. \n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "## Define the Nodes\n",
    "def llm_call(state: MessagesState) -> dict:\n",
    "    \"\"\"Execute LLM call with system prompt and message history.\n",
    "    \n",
    "    This function demonstrates context pruning by trimming messages to fit within\n",
    "    token limits while maintaining conversation coherence.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages\n",
    "    \"\"\"\n",
    "    # Add system prompt to the trimmed messages\n",
    "    messages = [SystemMessage(content=rag_prompt)] + state['messages']    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "## Improved prompt for context pruning\n",
    "tool_pruning_prompt = \"\"\"You are an expert at extracting relevant information from documents.\n",
    "\n",
    "Your task: Analyze the provided document and extract ONLY the information that directly answers or supports the user's specific request. Remove all irrelevant content.\n",
    "\n",
    "User's Request: {initial_request}\n",
    "\n",
    "Instructions for pruning:\n",
    "1. Keep information that directly addresses the user's question\n",
    "2. Preserve key facts, data, and examples that support the answer\n",
    "3. Remove tangential discussions, unrelated topics, and excessive background\n",
    "4. Maintain the logical flow and context of relevant information\n",
    "5. If multiple subtopics are discussed, focus only on those relevant to the request\n",
    "6. Preserve important quotes, statistics, and research findings when relevant\n",
    "\n",
    "Return the pruned content in a clear, concise format that maintains readability while focusing solely on what's needed to answer the user's request.\"\"\"\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: State) -> Literal[\"tool_node_with_pruning\", \"__end__\"]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node_with_pruning\"\n",
    "    \n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "def tool_node_with_pruning(state: State):\n",
    "    \"\"\"Performs the tool call with context pruning\"\"\"\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        \n",
    "        initial_request = state['messages'][0].content\n",
    "\n",
    "        # Prune the document content to focus on user's request\n",
    "        summarization_llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "        pruned_content = summarization_llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": tool_pruning_prompt.format(initial_request=initial_request)},\n",
    "            {\"role\": \"user\", \"content\": observation}\n",
    "        ])\n",
    "        \n",
    "        result.append(ToolMessage(content=pruned_content.content, tool_call_id=tool_call[\"id\"]))\n",
    "        \n",
    "    return {\"messages\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd5f280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Build the workflow\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "## Add the Nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node_with_pruning\", tool_node_with_pruning)\n",
    "\n",
    "## Add Edges to connect Nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node_with_pruning\": \"tool_node_with_pruning\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node_with_pruning\", \"llm_call\")\n",
    "\n",
    "## Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "## Display the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aebb920",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining Different Users based on Message Response\n",
    "from typing import Iterable, Any\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "def _role_of(msg: Any) -> str:\n",
    "    \"\"\"Infer a role string from LangChain message objects or dicts.\"\"\"\n",
    "    if isinstance(msg, HumanMessage):\n",
    "        return \"User\"\n",
    "    if isinstance(msg, AIMessage):\n",
    "        return \"RAG Agent\"\n",
    "    if isinstance(msg, SystemMessage):\n",
    "        return \"System\"\n",
    "    if isinstance(msg, ToolMessage):\n",
    "        return \"AI Tool\"\n",
    "    if isinstance(msg, BaseMessage):\n",
    "        ## Fallback for other BaseMessage subclasses\n",
    "        return getattr(msg, \"type\", msg.__class__.__name__.lower())\n",
    "    if isinstance(msg, dict):\n",
    "        return msg.get(\"role\", \"unknown\")\n",
    "    return msg.__class__.__name__.lower()\n",
    "\n",
    "def _content_of(msg: Any) -> str:\n",
    "    \"\"\"Extract textual content from LangChain message objects or dicts.\"\"\"\n",
    "    if isinstance(msg, BaseMessage):\n",
    "        c = msg.content\n",
    "        return c if isinstance(c, str) else str(c)\n",
    "    if isinstance(msg, dict):\n",
    "        return str(msg.get(\"content\", \"\"))\n",
    "    return str(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7182dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the format messages\n",
    "def format_messages(messages: Iterable[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Pretty-print a sequence of chat messages.\n",
    "    Supports LangChain BaseMessage subclasses and dicts with {'role','content'}.\n",
    "    Also prints AI tool calls if present.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "    for m in messages:\n",
    "        role = _role_of(m).capitalize()\n",
    "        content = _content_of(m)\n",
    "\n",
    "        # Handle AI tool calls (LangChain >= 0.2 exposes .tool_calls on AIMessage)\n",
    "        tool_calls_txt = \"\"\n",
    "        if isinstance(m, AIMessage):\n",
    "            tool_calls = getattr(m, \"tool_calls\", None)\n",
    "            if tool_calls:\n",
    "                parts = []\n",
    "                for tc in tool_calls:\n",
    "                    name = tc.get(\"name\") or tc.get(\"function\", {}).get(\"name\") or \"tool\"\n",
    "                    args = tc.get(\"args\") or tc.get(\"function\", {}).get(\"arguments\")\n",
    "                    parts.append(f\"- call {name}({args})\")\n",
    "                tool_calls_txt = (\"\\nTool calls:\\n\" + \"\\n\".join(parts)) if parts else \"\"\n",
    "\n",
    "        if isinstance(m, ToolMessage):\n",
    "            tool_name = getattr(m, \"name\", \"tool\")\n",
    "            lines.append(f\"Tool ({tool_name}): {content}\")\n",
    "        else:\n",
    "            lines.append(f\"{role}: {content}{tool_calls_txt}\")\n",
    "\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0c46fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: What are the types of reward hacking discussed in the blogs?\n",
      "Rag agent:  To provide a detailed response, I will first need to search through Lilian Weng's blog posts for information related to reward hacking. Here is the function call to retrieve relevant data:\n",
      "\n",
      "[{\"name\":\"retrieve_blog_posts\",\"arguments\":{\"query\":\"reward hacking\"}}]\n",
      "\n",
      "After analyzing the results, I found that Lilian Weng discusses several types of reward hacking in her blog posts:\n",
      "\n",
      "1. Game Hacking: This involves exploiting game mechanics to gain an unfair advantage or obtain rewards more easily. Examples include using cheats, trainers, or modifying game files.\n",
      "\n",
      "2. Cryptocurrency Mining Malware: Some forms of reward hacking target cryptocurrencies by infecting computers with malware that mines digital currencies without the user's knowledge or consent.\n",
      "\n",
      "3. Social Engineering Attacks: These attacks manipulate people into revealing sensitive information, such as login credentials or financial data, to gain access to rewards like accounts or funds.\n",
      "\n",
      "4. Bug Bounty Programs: This type of reward hacking encourages ethical hackers to find and report vulnerabilities in software applications for a monetary reward.\n",
      "\n",
      "5. Clickjacking: A technique used to trick users into clicking on hidden or covered links, often resulting in unwanted actions like generating ad revenue or stealing sensitive information.\n",
      "\n",
      "6. Account Takeover Attacks: These attacks aim to gain control over someone else's account by exploiting weak passwords, phishing scams, or other vulnerabilities. Once the attacker has access to the account, they can potentially collect rewards associated with it.\n"
     ]
    }
   ],
   "source": [
    "## Invoke the Agent\n",
    "query = \"What are the types of reward hacking discussed in the blogs?\"\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "\n",
    "## Format and display results\n",
    "print(format_messages(result[\"messages\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3356cb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea6acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
