{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72b60b01",
   "metadata": {},
   "source": [
    "## Context Pruning using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6749eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "## Using OpenAI/Thinking Machine Lab Lilian Weng's blog posts\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2025-05-01-thinking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\",\n",
    "    \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\",\n",
    "]\n",
    "\n",
    "## Load documents from each URL into docs object\n",
    "docs = [WebBaseLoader(url).load() for url in urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "## Flatten (Wide-format) the list of documents\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "\n",
    "## Initialize text splitter and specify chuck_size and chunk_overlap\n",
    "## For RAG we want to separate the information in the docs into blocks(chunks)\n",
    "## Then retrieve those blocks in context based on semantic similarity\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=2000, \n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "## Split documents into chunks\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e548f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import init_embeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "## Initialize embeddings model\n",
    "# providers = {\"mistralai\": \"langchain_mistralai\"}\n",
    "embeddings = init_embeddings(\"ollama:mistral:7b\")\n",
    "\n",
    "## Create in memory vector store from documents\n",
    "## This will live in memory and store these chunks for context\n",
    "vectorstore = InMemoryVectorStore.from_documents(\n",
    "    documents=doc_splits, \n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "## Create retriever from vector store >> Retrieves information from Memory VectorStore\n",
    "## 4 types of blog posts stored so it is important to store them separately\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473ded6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from rich.console import Console\n",
    "from rich.pretty import pprint\n",
    "\n",
    "## Initialize console for rich formatting\n",
    "console = Console()\n",
    "\n",
    "## Create retriever tool\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts.\",\n",
    ")\n",
    "\n",
    "## Test the retriever tool\n",
    "result = retriever_tool.invoke({\"query\": \"types of reward hacking\"})\n",
    "console.print(\"[bold green]Retriever Tool Results:[/bold green]\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382e286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "## Initialize language model\n",
    "llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "\n",
    "## Bind the tools\n",
    "tools = [retriever_tool]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "## Bind tools to LLM for agent functionality\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef2436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "from langgraph.graph import END, START, MessagesState, StateGraph\n",
    "\n",
    "## Define extended state with summary field\n",
    "class State(MessagesState):\n",
    "    \"\"\"Extended state that includes a summary field for context compression.\"\"\"\n",
    "    summary: str\n",
    "\n",
    "## Define the RAG agent system prompt\n",
    "rag_prompt = \"\"\"You are a helpful assistant tasked with retrieving information from a series of technical blog posts by Lilian Weng. \n",
    "Clarify the scope of research with the user before using your retrieval tool to gather context. Reflect on any context you fetch, and\n",
    "proceed until you have sufficient context to answer the user's research request.\"\"\"\n",
    "\n",
    "## Define the Nodes\n",
    "def llm_call(state: MessagesState) -> dict:\n",
    "    \"\"\"Execute LLM call with system prompt and message history.\n",
    "    \n",
    "    This function demonstrates context pruning by trimming messages to fit within\n",
    "    token limits while maintaining conversation coherence.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages\n",
    "    \"\"\"\n",
    "    # Add system prompt to the trimmed messages\n",
    "    messages = [SystemMessage(content=rag_prompt)] + state['messages']    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "## Improved prompt for context pruning\n",
    "tool_pruning_prompt = \"\"\"You are an expert at extracting relevant information from documents.\n",
    "\n",
    "Your task: Analyze the provided document and extract ONLY the information that directly answers or supports the user's specific request. Remove all irrelevant content.\n",
    "\n",
    "User's Request: {initial_request}\n",
    "\n",
    "Instructions for pruning:\n",
    "1. Keep information that directly addresses the user's question\n",
    "2. Preserve key facts, data, and examples that support the answer\n",
    "3. Remove tangential discussions, unrelated topics, and excessive background\n",
    "4. Maintain the logical flow and context of relevant information\n",
    "5. If multiple subtopics are discussed, focus only on those relevant to the request\n",
    "6. Preserve important quotes, statistics, and research findings when relevant\n",
    "\n",
    "Return the pruned content in a clear, concise format that maintains readability while focusing solely on what's needed to answer the user's request.\"\"\"\n",
    "\n",
    "# Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call\n",
    "def should_continue(state: State) -> Literal[\"tool_node_with_pruning\", \"__end__\"]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node_with_pruning\"\n",
    "    \n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "def tool_node_with_pruning(state: State):\n",
    "    \"\"\"Performs the tool call with context pruning\"\"\"\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        \n",
    "        initial_request = state['messages'][0].content\n",
    "\n",
    "        # Prune the document content to focus on user's request\n",
    "        summarization_llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "        pruned_content = summarization_llm.invoke([\n",
    "            {\"role\": \"system\", \"content\": tool_pruning_prompt.format(initial_request=initial_request)},\n",
    "            {\"role\": \"user\", \"content\": observation}\n",
    "        ])\n",
    "        \n",
    "        result.append(ToolMessage(content=pruned_content.content, tool_call_id=tool_call[\"id\"]))\n",
    "        \n",
    "    return {\"messages\": result}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd5f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the workflow\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "## Add the Nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node_with_pruning\", tool_node_with_pruning)\n",
    "\n",
    "## Add Edges to connect Nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node_with_pruning\": \"tool_node_with_pruning\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node_with_pruning\", \"llm_call\")\n",
    "\n",
    "## Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "## Display the agent\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c46fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invoke the Agent\n",
    "from utils import format_messages\n",
    "query = \"What are the types of reward hacking discussed in the blogs?\"\n",
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "format_messages(result['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3356cb68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea6acf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
