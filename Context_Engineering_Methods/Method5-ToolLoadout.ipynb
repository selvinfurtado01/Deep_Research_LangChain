{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef7f2c37",
   "metadata": {},
   "source": [
    "## Tool Loadout using LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844327ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing Necessary Modules\n",
    "import math  \n",
    "import types\n",
    "import uuid\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.embeddings import init_embeddings\n",
    "\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langgraph_bigtool.utils import convert_positional_only_function_to_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22185aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the language model for the agent\n",
    "llm = init_chat_model(\"ollama:mistral:7b\", temperature=0)\n",
    "\n",
    "## Define the tools\n",
    "## Extract and convert all mathematical functions from Python's math module\n",
    "all_tools = []\n",
    "for function_name in dir(math):\n",
    "    function = getattr(math, function_name)\n",
    "    \n",
    "    ## Only process built-in mathematical functions\n",
    "    if not isinstance(function, types.BuiltinFunctionType):\n",
    "        continue\n",
    "        \n",
    "    ## Convert math functions to LangChain tools (handles positional-only parameters)\n",
    "    if tool := convert_positional_only_function_to_tool(function):\n",
    "        all_tools.append(tool)\n",
    "\n",
    "## Create a tool registry mapping unique IDs to tool instances\n",
    "## This allows for efficient tool lookup and management\n",
    "tool_registry = {\n",
    "    str(uuid.uuid4()): tool\n",
    "    for tool in all_tools\n",
    "}\n",
    "\n",
    "## Set up vector store for semantic tool search\n",
    "## Uses embeddings to enable similarity-based tool selection\n",
    "embeddings = init_embeddings(\"ollama:mistral:7b\")\n",
    "\n",
    "## InMemory Store stores the data in memory and can be accessible throughout different threads\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,  ## embedding dimensions\n",
    "        \"fields\": [\"description\"],  ## Index tool descriptions for search\n",
    "    }\n",
    ")\n",
    "\n",
    "## Index all tools in the store for semantic similarity search\n",
    "for tool_id, tool in tool_registry.items():\n",
    "    store.put(\n",
    "        (\"tools\",),  ## Namespace for tool storage\n",
    "        tool_id,\n",
    "        {\n",
    "            \"description\": f\"{tool.name}: {tool.description}\",\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51a86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import standard and third-party packages\n",
    "from typing import Dict, Any\n",
    "from typing_extensions import Literal\n",
    "\n",
    "## LangChain core components\n",
    "from langchain_core.messages import SystemMessage, ToolMessage, HumanMessage\n",
    "\n",
    "## LangGraph components for workflow and state management\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.graph import END, START, StateGraph, MessagesState\n",
    "\n",
    "## Jupyter display utilities\n",
    "from IPython.display import Image, display\n",
    "\n",
    "## Extended state class to store dynamically selected tools\n",
    "class ToolLoadoutState(MessagesState):\n",
    "    \"\"\"State that extends MessagesState to include dynamically selected tools.\n",
    "    \n",
    "    This allows the agent to maintain context about which tools are currently\n",
    "    available and bound to the conversation.\n",
    "    \"\"\"\n",
    "    tools_by_name: Dict[str, Any] = {}\n",
    "\n",
    "\n",
    "## Define the Nodes\n",
    "## System prompt defining the agent's role and capabilities\n",
    "system_prompt = \"\"\"You are a helpful assistant with access to mathematical functions from Python's math library. \n",
    "You can search for and use relevant mathematical tools to solve problems. \n",
    "When you need to perform mathematical calculations, first determine what type of mathematical operation you need, \n",
    "then use the appropriate tools from the math library.\"\"\"\n",
    "\n",
    "def llm_call(state: ToolLoadoutState, store: BaseStore) -> dict:\n",
    "    \"\"\"Main LLM call that dynamically selects and binds relevant tools.\n",
    "    \n",
    "    This function implements the core tool loadout pattern:\n",
    "    1. Extract query context from user message\n",
    "    2. Search for semantically relevant tools  \n",
    "    3. Bind only relevant tools to the LLM\n",
    "    4. Generate response with focused tool set\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state containing messages and tools\n",
    "        store: Vector store containing indexed tool descriptions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with new messages and updated tool registry\n",
    "    \"\"\"\n",
    "    # Extract user query for semantic tool search\n",
    "    messages = state[\"messages\"]\n",
    "    if messages and isinstance(messages[-1], HumanMessage):\n",
    "        query = messages[-1].content\n",
    "    else:\n",
    "        query = \"mathematical calculation\"  ## Default fallback\n",
    "    \n",
    "    ## Perform semantic similarity search to find relevant tools\n",
    "    search_results = store.search((\"tools\",), query=query, limit=5)\n",
    "    \n",
    "    ## Build focused tool set from search results\n",
    "    relevant_tools = []\n",
    "    tools_by_name = {}\n",
    "    \n",
    "    for result in search_results:\n",
    "        tool_id = result.key\n",
    "        if tool_id in tool_registry:\n",
    "            tool = tool_registry[tool_id]\n",
    "            relevant_tools.append(tool)\n",
    "            tools_by_name[tool.name] = tool\n",
    "    \n",
    "    ## Bind only relevant tools to avoid context overload\n",
    "    llm_with_tools = llm.bind_tools(relevant_tools) if relevant_tools else llm\n",
    "    \n",
    "    ## Generate response with focused context\n",
    "    response = llm_with_tools.invoke(\n",
    "        [SystemMessage(content=system_prompt)] + state[\"messages\"]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [response],\n",
    "        \"tools_by_name\": tools_by_name\n",
    "    }\n",
    "\n",
    "def tool_node(state: ToolLoadoutState) -> dict:\n",
    "    \"\"\"Execute tool calls using the dynamically selected tool set.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state with tool calls\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with tool execution results\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        ## Retrieve tool from the focused set stored in state\n",
    "        tool = state[\"tools_by_name\"][tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=str(observation), tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}\n",
    "\n",
    "def should_continue(state: ToolLoadoutState) -> Literal[\"tool_node\", \"__end__\"]:\n",
    "    \"\"\"Determine workflow continuation based on tool calls.\n",
    "    \n",
    "    Args:\n",
    "        state: Current conversation state\n",
    "        \n",
    "    Returns:\n",
    "        Next node name or END\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    ## Continue to tool execution if LLM made tool calls\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "    \n",
    "    ## Otherwise end the conversation\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaee7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build the tool loadout workflow\n",
    "agent_builder = StateGraph(ToolLoadoutState)\n",
    "\n",
    "## Add the nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "## Define the Edges\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tool_node\": \"tool_node\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "## Compile the agent with tool store for semantic search\n",
    "agent = agent_builder.compile(store=store)\n",
    "\n",
    "## Display the workflow \n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a66af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Invoke the Agent\n",
    "from utils import format_messages\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "query = \"Use available tools to calculate arc cosine of 0.5.\"\n",
    "result = agent.invoke({\"messages\": [HumanMessage(content=query)]})\n",
    "format_messages(result['messages']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bde2bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3caafadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01d404e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910fb008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987d776d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc3c37c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9534bfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2d4e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f8412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909b3db0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f60fa75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ce877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c1f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1abf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e278454e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b974ddcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11caff62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15610c75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
